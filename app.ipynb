{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562f2596",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA): Bitext Retail Banking LLM Chatbot Training Dataset\n",
    "\n",
    "This notebook performs **exploratory analysis and cleaning** for the Bitext retail banking dataset loaded directly from Hugging Face as a parquet file.\n",
    "\n",
    "We will:\n",
    "- Inspect the dataset (shape, columns, sample rows)\n",
    "- Check **missing values** (including *blank strings*)\n",
    "- Check **duplicates**\n",
    "- Do **basic cleaning** (standardize columns, remove blanks, trim whitespace, drop duplicates)\n",
    "- Explore **category/intent distribution** (class balance)\n",
    "- Explore **text length statistics** (question/answer word counts)\n",
    "- Extract **common n-grams** to understand vocabulary\n",
    "- Save a cleaned copy for modeling\n",
    "\n",
    "> Dataset source (parquet):  \n",
    "> `hf://datasets/bitext/Bitext-retail-banking-llm-chatbot-training-dataset/bitext-retail-banking-llm-chatbot-training-dataset.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c103a",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "If you run this locally and you're missing packages, install them (uncomment the `pip install` line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa331ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if needed:\n",
    "# %pip install pandas pyarrow fsspec huggingface-hub scikit-learn matplotlib\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e878b",
   "metadata": {},
   "source": [
    "## 1) Load the dataset\n",
    "\n",
    "Using your exact loading snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7231814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    \"hf://datasets/bitext/Bitext-retail-banking-llm-chatbot-training-dataset/\"\n",
    "    \"bitext-retail-banking-llm-chatbot-training-dataset.parquet\"\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0478a52",
   "metadata": {},
   "source": [
    "## 2) Quick overview (shape, columns, types)\n",
    "\n",
    "This helps confirm what's inside the parquet and the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows, Columns:\", df.shape)\n",
    "display(pd.DataFrame({\"column\": df.columns, \"dtype\": [str(t) for t in df.dtypes]}))\n",
    "\n",
    "df.describe(include=\"all\").T.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad791462",
   "metadata": {},
   "source": [
    "## 3) Identify likely text / label columns\n",
    "\n",
    "Different HF datasets use different names (e.g., `instruction/response`, `utterance/answer`, etc.).\n",
    "This cell attempts to auto-detect:\n",
    "- `question` (user input)\n",
    "- `answer` (assistant output)\n",
    "- `category` (department/topic label)\n",
    "- `intent` (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_CANDIDATES = [\"instruction\", \"question\", \"utterance\", \"query\", \"input\", \"prompt\", \"text\", \"user\"]\n",
    "ANSWER_CANDIDATES = [\"response\", \"answer\", \"assistant\", \"output\", \"completion\", \"bot\"]\n",
    "CATEGORY_CANDIDATES = [\"category\", \"dept\", \"department\", \"topic\", \"label\"]\n",
    "INTENT_CANDIDATES = [\"intent\", \"scenario\", \"intent_name\"]\n",
    "\n",
    "def pick_col(columns, candidates):\n",
    "    cols_lower = {c.lower(): c for c in columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols_lower:\n",
    "            return cols_lower[cand.lower()]\n",
    "    # fallback heuristic\n",
    "    for c in columns:\n",
    "        cl = c.lower()\n",
    "        if any(k in cl for k in [\"question\", \"instruction\", \"utterance\", \"query\", \"prompt\", \"text\", \"input\"]):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "qcol = pick_col(df.columns, TEXT_CANDIDATES)\n",
    "acol = pick_col(df.columns, ANSWER_CANDIDATES)\n",
    "catcol = pick_col(df.columns, CATEGORY_CANDIDATES)\n",
    "intentcol = pick_col(df.columns, INTENT_CANDIDATES)\n",
    "\n",
    "print(\"Detected columns:\")\n",
    "print(\"  question:\", qcol)\n",
    "print(\"  answer:  \", acol)\n",
    "print(\"  category:\", catcol)\n",
    "print(\"  intent:  \", intentcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56108fa7",
   "metadata": {},
   "source": [
    "## 4) Missing values (including blank strings)\n",
    "\n",
    "For text datasets, **blanks** often matter as much as `NaN`.\n",
    "This section checks:\n",
    "- True missing (`NaN`)\n",
    "- Blank strings like `\"\"` or `\"   \"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blank(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    # pandas NA\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x).strip() == \"\"\n",
    "\n",
    "missing_summary = []\n",
    "for c in df.columns:\n",
    "    na_rate = df[c].isna().mean()\n",
    "    blank_rate = df[c].apply(is_blank).mean() if df[c].dtype == \"object\" else np.nan\n",
    "    missing_summary.append({\n",
    "        \"column\": c,\n",
    "        \"na_%\": round(na_rate * 100, 2),\n",
    "        \"blank_%\": round(blank_rate * 100, 2) if not np.isnan(blank_rate) else None\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary).sort_values([\"na_%\",\"blank_%\"], ascending=False)\n",
    "display(missing_df)\n",
    "\n",
    "# Quick bar chart of blank% for object columns (if any)\n",
    "obj_missing = missing_df.dropna(subset=[\"blank_%\"]).copy()\n",
    "if not obj_missing.empty:\n",
    "    fig = plt.figure()\n",
    "    plt.bar(obj_missing[\"column\"], obj_missing[\"blank_%\"])\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"Blank %\")\n",
    "    plt.title(\"Blank strings per column (object columns)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f1a9",
   "metadata": {},
   "source": [
    "## 5) Duplicates\n",
    "\n",
    "We check:\n",
    "- Duplicate rows (exact duplicates across all columns)\n",
    "- Duplicate questions (common in prompt-response datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_rows = df.duplicated().mean()\n",
    "print(f\"Exact duplicate rows: {dup_rows:.2%}\")\n",
    "\n",
    "if qcol:\n",
    "    dup_q = df[qcol].astype(str).duplicated().mean()\n",
    "    print(f\"Duplicate questions in '{qcol}': {dup_q:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb13f5c",
   "metadata": {},
   "source": [
    "## 6) Build a clean modeling dataframe\n",
    "\n",
    "We standardize to a consistent schema:\n",
    "\n",
    "- `question` (string)\n",
    "- `answer` (string)\n",
    "- `category` (UPPERCASE label if available; otherwise 'CONTACT')\n",
    "- `intent` (optional; blank if not available)\n",
    "\n",
    "Cleaning steps:\n",
    "- Convert to strings\n",
    "- Strip whitespace & collapse multiple spaces\n",
    "- Drop rows where question/answer are blank\n",
    "- Drop duplicates (question+answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99756d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not qcol or not acol:\n",
    "    raise ValueError(f\"Could not detect question/answer columns. Found columns: {list(df.columns)}\")\n",
    "\n",
    "base = df.copy()\n",
    "\n",
    "base = base.rename(columns={\n",
    "    qcol: \"question\",\n",
    "    acol: \"answer\",\n",
    "    (catcol if catcol else \"___no_category___\"): \"category\",\n",
    "    (intentcol if intentcol else \"___no_intent___\"): \"intent\",\n",
    "})\n",
    "\n",
    "# Ensure required columns exist\n",
    "if \"category\" not in base.columns:\n",
    "    base[\"category\"] = \"CONTACT\"\n",
    "if \"intent\" not in base.columns:\n",
    "    base[\"intent\"] = \"\"\n",
    "\n",
    "# Normalize text\n",
    "def norm_text(s):\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "base[\"question\"] = base[\"question\"].apply(norm_text)\n",
    "base[\"answer\"] = base[\"answer\"].apply(norm_text)\n",
    "\n",
    "# Category / intent cleanup\n",
    "base[\"category\"] = base[\"category\"].astype(str).apply(norm_text)\n",
    "base[\"category\"] = base[\"category\"].replace({\"\": \"CONTACT\"}).str.upper()\n",
    "\n",
    "base[\"intent\"] = base[\"intent\"].astype(str).apply(norm_text)\n",
    "# keep intent as-is (case can matter), but replace pure blanks to \"\"\n",
    "base.loc[base[\"intent\"].str.strip() == \"\", \"intent\"] = \"\"\n",
    "\n",
    "# Drop blank Q/A\n",
    "before = len(base)\n",
    "base = base[(base[\"question\"].str.strip() != \"\") & (base[\"answer\"].str.strip() != \"\")]\n",
    "after_drop_blank = len(base)\n",
    "\n",
    "# Drop duplicates\n",
    "base = base.drop_duplicates(subset=[\"question\", \"answer\"]).reset_index(drop=True)\n",
    "after_dedup = len(base)\n",
    "\n",
    "print(\"Rows before cleaning:\", before)\n",
    "print(\"After dropping blank Q/A:\", after_drop_blank)\n",
    "print(\"After dropping duplicate Q/A:\", after_dedup)\n",
    "\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846f8c03",
   "metadata": {},
   "source": [
    "## 7) Re-check missing values after cleaning\n",
    "\n",
    "We re-run missing checks for the new `base` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_after = []\n",
    "for c in base.columns:\n",
    "    na_rate = base[c].isna().mean()\n",
    "    blank_rate = base[c].apply(is_blank).mean() if base[c].dtype == \"object\" else np.nan\n",
    "    missing_after.append({\n",
    "        \"column\": c,\n",
    "        \"na_%\": round(na_rate * 100, 2),\n",
    "        \"blank_%\": round(blank_rate * 100, 2) if not np.isnan(blank_rate) else None\n",
    "    })\n",
    "\n",
    "missing_after_df = pd.DataFrame(missing_after).sort_values([\"na_%\",\"blank_%\"], ascending=False)\n",
    "display(missing_after_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142c618",
   "metadata": {},
   "source": [
    "## 8) Category distribution (class balance)\n",
    "\n",
    "This is critical for a chatbot routing/classification task:\n",
    "- If some categories are rare, you may need **class weighting**, **sampling**, or **merging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_counts = base[\"category\"].value_counts().rename_axis(\"category\").reset_index(name=\"count\")\n",
    "display(cat_counts)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.bar(cat_counts[\"category\"], cat_counts[\"count\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Examples per category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show rare categories (adjust threshold if needed)\n",
    "rare_threshold = 50\n",
    "rare = cat_counts[cat_counts[\"count\"] < rare_threshold]\n",
    "print(f\"Rare categories (<{rare_threshold} examples):\", len(rare))\n",
    "display(rare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b69011",
   "metadata": {},
   "source": [
    "## 9) Intent distribution (if available)\n",
    "\n",
    "Some versions include an `intent`/scenario column. We inspect it if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476eee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"intent\" in base.columns and (base[\"intent\"].str.strip() != \"\").any():\n",
    "    intent_counts = base[base[\"intent\"].str.strip() != \"\"][\"intent\"].value_counts().head(30)\n",
    "    display(intent_counts.rename_axis(\"intent\").reset_index(name=\"count\"))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    intent_counts.head(20).plot(kind=\"bar\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Top 20 intents\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No non-empty intent values found (or intent column not present).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4f0f0",
   "metadata": {},
   "source": [
    "## 10) Text length analysis (words & characters)\n",
    "\n",
    "This helps you decide:\n",
    "- truncation strategy (if you train models)\n",
    "- detect outliers (very long/very short prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd8ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(s):\n",
    "    return len(re.findall(r\"[A-Za-z0-9']+\", str(s).lower()))\n",
    "\n",
    "base[\"_q_words\"] = base[\"question\"].apply(word_count)\n",
    "base[\"_a_words\"] = base[\"answer\"].apply(word_count)\n",
    "base[\"_q_chars\"] = base[\"question\"].astype(str).apply(len)\n",
    "base[\"_a_chars\"] = base[\"answer\"].astype(str).apply(len)\n",
    "\n",
    "display(base[[\"_q_words\",\"_a_words\",\"_q_chars\",\"_a_chars\"]].describe(percentiles=[.5,.9,.95,.99]).T)\n",
    "\n",
    "# histograms (clipped at p99 to avoid a single huge outlier ruining the plot)\n",
    "q_clip = base[\"_q_words\"].clip(upper=base[\"_q_words\"].quantile(0.99))\n",
    "a_clip = base[\"_a_words\"].clip(upper=base[\"_a_words\"].quantile(0.99))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(q_clip, bins=50)\n",
    "plt.title(\"Question length (words) — clipped at p99\")\n",
    "plt.xlabel(\"Words\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(a_clip, bins=50)\n",
    "plt.title(\"Answer length (words) — clipped at p99\")\n",
    "plt.xlabel(\"Words\"); plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780bc2d",
   "metadata": {},
   "source": [
    "## 11) Inspect outliers and edge cases\n",
    "\n",
    "We print:\n",
    "- shortest questions\n",
    "- longest questions\n",
    "- longest answers\n",
    "\n",
    "This helps you spot data artifacts and decide if additional cleaning is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40381f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(df, col, n=8, ascending=True, title=\"\"):\n",
    "    sub = df.sort_values(col, ascending=ascending).head(n)[[\"category\",\"question\",\"answer\",col]]\n",
    "    st = f\"\\n{title}\\n\" if title else \"\"\n",
    "    print(st)\n",
    "    display(sub)\n",
    "\n",
    "show_examples(base, \"_q_words\", n=10, ascending=True, title=\"Shortest questions\")\n",
    "show_examples(base, \"_q_words\", n=10, ascending=False, title=\"Longest questions\")\n",
    "show_examples(base, \"_a_words\", n=10, ascending=False, title=\"Longest answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae85496",
   "metadata": {},
   "source": [
    "## 12) Common n-grams in questions (overall)\n",
    "\n",
    "This is a simple way to understand what customers ask about most often.\n",
    "We compute top unigrams + bigrams using `CountVectorizer`.\n",
    "\n",
    "> Tip: If you see sensitive terms (PIN/password), you can later add safety filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = base[\"question\"].astype(str).tolist()\n",
    "\n",
    "vec = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=10)\n",
    "X = vec.fit_transform(texts)\n",
    "counts = np.asarray(X.sum(axis=0)).ravel()\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "topn = 30\n",
    "idx = counts.argsort()[-topn:][::-1]\n",
    "\n",
    "ngrams = pd.DataFrame({\"ngram\": vocab[idx], \"count\": counts[idx]})\n",
    "display(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab716c",
   "metadata": {},
   "source": [
    "## 13) N-grams by category (top categories)\n",
    "\n",
    "For routing/classification, it’s useful to see the phrases that characterize each category.\n",
    "We show top n-grams for the **top 5 largest categories**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec89bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats = base[\"category\"].value_counts().head(5).index.tolist()\n",
    "results = {}\n",
    "\n",
    "for cat in top_cats:\n",
    "    subset = base[base[\"category\"] == cat][\"question\"].astype(str).tolist()\n",
    "    if len(subset) < 50:\n",
    "        continue\n",
    "    v = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=5)\n",
    "    Xc = v.fit_transform(subset)\n",
    "    counts = np.asarray(Xc.sum(axis=0)).ravel()\n",
    "    vocab = np.array(v.get_feature_names_out())\n",
    "    idx = counts.argsort()[-20:][::-1]\n",
    "    results[cat] = pd.DataFrame({\"ngram\": vocab[idx], \"count\": counts[idx]})\n",
    "\n",
    "for cat, table in results.items():\n",
    "    print(f\"\\nTop n-grams for category: {cat}\")\n",
    "    display(table.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b2b6b",
   "metadata": {},
   "source": [
    "## 14) (Optional) TF-IDF keywords per category\n",
    "\n",
    "TF‑IDF highlights terms that are more unique to a category compared to the whole dataset.\n",
    "This can help you:\n",
    "- justify why categories are separable\n",
    "- create keyword rules for a baseline router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=5, max_df=0.9)\n",
    "X = tfidf.fit_transform(base[\"question\"].astype(str))\n",
    "features = np.array(tfidf.get_feature_names_out())\n",
    "\n",
    "def top_tfidf_for_category(cat, k=15):\n",
    "    idx = base.index[base[\"category\"] == cat].to_numpy()\n",
    "    if len(idx) == 0:\n",
    "        return pd.DataFrame(columns=[\"term\",\"score\"])\n",
    "    v = np.asarray(X[idx].mean(axis=0)).ravel()\n",
    "    top = v.argsort()[-k:][::-1]\n",
    "    return pd.DataFrame({\"term\": features[top], \"score\": v[top]})\n",
    "\n",
    "for cat in top_cats:\n",
    "    print(f\"\\nTF‑IDF keywords for: {cat}\")\n",
    "    display(top_tfidf_for_category(cat, k=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c9356",
   "metadata": {},
   "source": [
    "## 15) Save the cleaned dataset\n",
    "\n",
    "You can now save `base` for downstream tasks:\n",
    "- training a TF‑IDF retrieval bot\n",
    "- training a classifier for routing\n",
    "- creating train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1242d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally\n",
    "base.to_parquet(\"bitext_retail_banking_clean.parquet\", index=False)\n",
    "base.to_csv(\"bitext_retail_banking_clean.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - bitext_retail_banking_clean.parquet\")\n",
    "print(\" - bitext_retail_banking_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40266c6",
   "metadata": {},
   "source": [
    "## 16) Key takeaways (write these into your assignment)\n",
    "\n",
    "When you finish running this notebook, summarize:\n",
    "- Whether there were missing values / blank strings\n",
    "- Whether duplicates existed and what you removed\n",
    "- Category balance (which categories dominate, which are rare)\n",
    "- Typical question/answer lengths (median and long-tail)\n",
    "- Example vocabulary signals (top n-grams / TF-IDF terms)\n",
    "- What cleaning decisions you made and why\n",
    "\n",
    "**Next step suggestions**:\n",
    "- If class imbalance is strong: use class weights, stratified splits, or merge rare categories.\n",
    "- If many very short questions exist: include bigrams, maybe character n-grams.\n",
    "- If outliers are extreme: cap length or remove noisy rows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
